{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMID/PCvEkE+EiMXO4cqTCU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Munna-Prasad-Gupta/C/blob/main/22181.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73I7PGV9by6d",
        "outputId": "8be1d65a-a34a-4c27-c8af-acab7ce716f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello World\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write a python function that takes text document as input and returns a dictionary containing the frequency of each word in the document\n",
        "\n",
        "from collections import Counter\n",
        "def word_frequency(file_path):\n",
        "    try:\n",
        "      with open(file_path,'r') as file:\n",
        "        content = file.read()\n",
        "        words = content.split()\n",
        "        frequency = Counter(words)\n",
        "        return frequency\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{file_path}' not found.\")\n",
        "\n",
        "\n",
        "freq = word_frequency('sample.txt')\n",
        "for word,count in freq.items():\n",
        "  print(f\"word is {word} and count is {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_4lDTD6b4Ks",
        "outputId": "42e86573-54db-49dc-d6c7-bde1d630ec88"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word is B. and count is 1\n",
            "word is Causes and count is 1\n",
            "word is of and count is 5\n",
            "word is Stuttering and count is 1\n",
            "word is The and count is 1\n",
            "word is origins and count is 1\n",
            "word is stuttering and count is 2\n",
            "word is are and count is 3\n",
            "word is complex, and count is 1\n",
            "word is involving and count is 1\n",
            "word is genetics, and count is 1\n",
            "word is neurobiology, and count is 1\n",
            "word is and and count is 6\n",
            "word is environmental and count is 1\n",
            "word is influences and count is 1\n",
            "word is [4]. and count is 1\n",
            "word is There and count is 1\n",
            "word is clear and count is 1\n",
            "word is genetic and count is 1\n",
            "word is links and count is 1\n",
            "word is within and count is 1\n",
            "word is families, and count is 1\n",
            "word is while and count is 1\n",
            "word is on and count is 2\n",
            "word is a and count is 3\n",
            "word is neurological and count is 1\n",
            "word is level, and count is 1\n",
            "word is differences and count is 1\n",
            "word is in and count is 3\n",
            "word is speech and count is 1\n",
            "word is processing and count is 1\n",
            "word is neural and count is 1\n",
            "word is pathways and count is 1\n",
            "word is can and count is 2\n",
            "word is be and count is 1\n",
            "word is observed and count is 1\n",
            "word is [5]. and count is 1\n",
            "word is Environmental and count is 1\n",
            "word is factors, and count is 1\n",
            "word is including and count is 1\n",
            "word is lan\u0002guage and count is 1\n",
            "word is development and count is 2\n",
            "word is family and count is 1\n",
            "word is dynamics, and count is 1\n",
            "word is also and count is 1\n",
            "word is play and count is 1\n",
            "word is signif\u0002icant and count is 1\n",
            "word is role and count is 1\n",
            "word is the and count is 1\n",
            "word is stuttering, and count is 1\n",
            "word is heightened and count is 1\n",
            "word is stress and count is 1\n",
            "word is emotions and count is 1\n",
            "word is worsen and count is 1\n",
            "word is its and count is 1\n",
            "word is symptoms. and count is 1\n",
            "word is This and count is 1\n",
            "word is intri\u0002cate and count is 1\n",
            "word is web and count is 1\n",
            "word is causative and count is 1\n",
            "word is factors and count is 1\n",
            "word is makes and count is 1\n",
            "word is challenging and count is 1\n",
            "word is condition and count is 1\n",
            "word is to and count is 2\n",
            "word is fully and count is 1\n",
            "word is comprehend, and count is 1\n",
            "word is thus, and count is 1\n",
            "word is ongoing and count is 1\n",
            "word is research and count is 1\n",
            "word is endeavors and count is 1\n",
            "word is dedicated and count is 1\n",
            "word is deepening and count is 1\n",
            "word is our and count is 1\n",
            "word is understanding and count is 1\n",
            "word is 979-8-3503-5813-1/23/$31.00 and count is 1\n",
            "word is Â© and count is 1\n",
            "word is 2023 and count is 2\n",
            "word is IEEE and count is 1\n",
            "word is 525 and count is 1\n",
            "word is 5th and count is 1\n",
            "word is International and count is 1\n",
            "word is Conference and count is 1\n",
            "word is Advancements and count is 1\n",
            "word is Computing and count is 1\n",
            "word is (ICAC) and count is 1\n",
            "word is | and count is 1\n",
            "word is 979-8-3503-5813-1/23/$3 and count is 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def word_frequency_counter(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation and other non-alphanumeric characters\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "\n",
        "    # Split the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Count the frequency of each word\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    return dict(word_freq)\n",
        "\n",
        "\n",
        "    text_document = \"\"\"\n",
        "    This is a sample text document. It contains some words. Some words appear more than once.\n",
        "    This is to test the word frequency counter. Let's see how many times each word appears.\n",
        "    \"\"\"\n",
        "\n",
        "    word_freq_dict = word_frequency_counter(text_document)\n",
        "    print(word_freq_dict)"
      ],
      "metadata": {
        "id": "v5qLPVwdde-U"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function to clean and tokenize a given text removing punctuation and converting words to lower case\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def tokenize(file_path):\n",
        "  try:\n",
        "    with open(file_path,'r') as file:\n",
        "      content = file.read()\n",
        "      content = content.lower()\n",
        "      content = re.sub(r'[^\\w\\s]','',content)\n",
        "      tokens = word_tokenize(content)\n",
        "      return tokens\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File '{file_path}' not found.\")\n",
        "\n",
        "tokens = tokenize('intro.txt')\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS4pIKK4g5Xn",
        "outputId": "5bc9fe3b-1c6b-4c6e-f9c2-a5faee2b926f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'introduction', 'parts', 'of', 'speech', 'pos', 'tagging', 'is', 'essential', 'to', 'many', 'facets', 'of', 'natural', 'language', 'processing', 'nlp', 'significantly', 'contributing', 'for', 'syntactic', 'and', 'semantic', 'analysis', 'in', 'the', 'process', 'understanding', 'sentence', 'structure', 'is', 'the', 'goal', 'of', 'syntax', 'analysis', 'and', 'pos', 'tags', 'play', 'a', 'crucial', 'role', 'in', 'this', 'process', 'by', 'clarifying', 'the', 'grammatical', 'functions', 'of', 'individual', 'words', 'these', 'tags', 'provide', 'information', 'about', 'sentence', 'structure', 'including', 'subjectverbobject', 'relationships', 'and', 'help', 'with', 'understanding', 'how', 'words', 'work', 'together', 'within', 'a', 'sentence', 'conversely', 'semantic', 'analysis', 'explores', 'the', 'meaning', 'of', 'individual', 'words', 'and', 'sentences', 'pos', 'tags', 'provide', 'useful', 'indicators', 'for', 'semantic', 'analysis', 'by', 'highlighting', 'the', 'word', 'grammatical', 'categories', 'for', 'example', 'distinguishing', 'between', 'nouns', 'and', 'verbs', 'can', 'have', 'a', 'significant', 'impact', 'on', 'how', 'one', 'understands', 'the', 'meaning', 'of', 'a', 'sentence', 'by', 'illuminating', 'the', 'subjects', 'actions', 'or', 'objects', '3', 'additionally', 'pos', 'tagging', 'helps', 'to', 'resolve', 'the', 'ambiguity', 'that', 'exists', 'in', 'natural', 'language', 'many', 'words', 'have', 'more', 'than', 'one', 'meaning', 'or', 'can', 'perform', 'differently', 'depending', 'on', 'the', 'situation', 'it', 'is', 'possible', 'to', 'reduce', 'ambiguity', 'and', 'improve', 'comprehension', 'and', 'interpretation', 'of', 'text', 'by', 'giving', 'words', 'the', 'proper', 'pos', 'tags', 'to', 'produce', 'meaningful', 'insights', 'from', 'text', 'data', 'language', 'understanding', 'tasks', 'such', 'as', 'machine', 'translation', 'sentiment', 'analysis', 'and', 'named', 'entity', 'recognition', 'mainly', 'depend', 'on', 'accurate', 'pointofsale', 'pos', 'information', 'for', 'this', 'reason', 'this', 'process', 'is', 'essential', 'furthermore', 'by', 'making', 'it', 'possible', 'to', 'identify', 'linguistic', 'components', 'like', 'proper', 'nouns', 'verb', 'phrases', 'or', 'adjectival', 'modifiers', 'pos', 'tags', 'aid', 'in', 'the', 'extraction', 'of', 'information', 'moreover', 'pos', 'tags', 'are', 'useful', 'features', 'for', 'training', 'models', 'in', 'machine', 'learning', 'applications', 'improving', 'their', 'performance', 'by', 'offering', 'crucial', 'contextual', 'information', 'for', 'more', 'accurate', 'predictions', 'in', 'various', 'nlp', 'tasks', '7', 'previous', 'literature', 'has', 'highlighted', 'the', 'challenges', 'associated', 'with', 'hmmbased', 'pos', 'tagging', 'especially', 'when', 'dealing', 'with', 'ambiguous', 'terms', 'and', 'complicated', 'sentence', 'patterns', '1', 'furthermore', 'there', 'is', 'a', 'growing', 'need', 'for', 'more', 'reliable', 'and', 'accurate', 'part', 'of', 'speech', 'pos', 'systems', 'due', 'to', 'the', 'growing', 'demand', 'for', 'nlp', 'systems', 'to', 'analyse', 'heterogeneous', 'and', 'unstructured', 'text', 'input2', 'this', 'study', 'intends', 'to', 'investigate', 'advanced', 'pos', 'tagging', 'techniques', 'specifically', 'conditional', 'random', 'fields', 'crfs', '10', 'and', 'their', 'integration', 'with', 'the', 'spacy', 'nlp', 'toolkit', 'motivated', 'by', 'the', 'inadequacies', 'of', 'existing', 'systems', 'crfs', 'provide', 'improved', 'sequence', 'modelling', 'capabilities', 'making', 'it', 'possible', 'to', 'include', 'contextual', 'data', 'more', 'effectively', 'and', 'depends', 'on', 'features', '2', 'integrating', 'with', 'spacy', 'makes', 'use', 'of', 'its', 'extensive', 'linguistic', 'features', 'and', 'domainspecific', 'expertise', 'to', 'further', 'improve', 'the', 'tagging', 'process', '15', 'this', 'paper', 'aims', 'to', 'analyse', 'two', 'things', '1', 'how', 'well', 'crfs', 'work', 'in', 'comparison', 'to', 'conventional', 'hmmbased', 'pos', 'tagging', 'techniques', 'and', '2', 'how', 'combining', 'crfs', 'with', 'spacy', 'affects', 'the', 'resilience', 'and', 'accuracy', 'of', 'pos', 'tagging', 'this', 'work', 'contributes', 'by', 'showing', 'the', 'additional', 'advantages', 'of', 'integrating', 'crfs', 'with', 'spacy', 'and', 'by', 'offering', 'actual', 'proof', 'of', 'the', 'superiority', 'of', 'crfs', 'over', 'hmms', 'for', 'pos', 'tagging', 'furthermore', 'we', 'provide', 'insights', 'into', 'the', 'difficulties', 'and', 'possibilities', 'of', 'utilising', 'cuttingedge', 'nlp', 'approaches', 'for', 'enhanced', 'linguistic', 'examination', '9', 'the', 'rest', 'of', 'this', 'paper', 'is', 'structured', 'as', 'follows', 'a', 'thorough', 'summary', 'of', 'relevant', 'work', 'in', 'the', 'field', 'of', 'pos', 'tagging', 'is', 'given', 'in', 'section', '2', 'the', 'approach', 'is', 'covered', 'in', 'section', '3', 'along', 'with', 'specifics', 'on', 'how', 'crfs', 'are', 'implemented', 'and', 'integrated', 'with', 'spacy', '16', 'the', 'experimental', 'setup', 'and', 'data', 'analysis', 'are', 'shown', 'in', 'section', '4', 'and', 'a', 'discussion', 'is', 'included', 'in', 'section', '5', 'section', '6', 'offers', 'some', 'final', 'thoughts', 'on', 'potential', 'directions', 'for', 'future', 'research', 'to', 'wrap', 'up', 'the', 'paper', '2024', 'third', 'international', 'conference', 'on', 'distributed', 'computing', 'and', 'electrical', 'circuits', 'and', 'electronics', 'icdcece', '9798350318609243100', '2024', 'ieee', 'doi', '101109icdcece60827202410548901', 'authorized', 'licensed', 'use', 'limited', 'to', 'amrita', 'school', 'of', 'engineering', 'downloaded', 'on', 'july', '232024', 'at', '052829', 'utc', 'from', 'ieee', 'xplore', 'restrictions', 'apply', 'fig', '1', 'part', 'of', 'speech', 'tagging', 'fig1', 'shows', 'an', 'example', 'of', 'how', 'parts', 'of', 'speech', 'tagging', 'is', 'done', 'for', 'as', 'sentence']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# develop a python function that removes stop words from a given sentence or a text\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def remove_stopwords(file_path):\n",
        "  nltk.download('stopwords')\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  try:\n",
        "    with open(file_path,'r') as file:\n",
        "      content = file.read()\n",
        "      content = content.lower()\n",
        "      content = re.sub(r'[^\\w\\s]','',content)\n",
        "      words = word_tokenize(content)\n",
        "      filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "      return filtered_words\n",
        "  except FileNotFoundError:\n",
        "    print(f\"File '{file_path}' not found.\")\n",
        "\n",
        "filtered_words = remove_stopwords('intro.txt')\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej6k-m7-iTn-",
        "outputId": "80681d5c-8639-4a68-baa4-4a3cb1a7851a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['introduction', 'parts', 'speech', 'pos', 'tagging', 'essential', 'many', 'facets', 'natural', 'language', 'processing', 'nlp', 'significantly', 'contributing', 'syntactic', 'semantic', 'analysis', 'process', 'understanding', 'sentence', 'structure', 'goal', 'syntax', 'analysis', 'pos', 'tags', 'play', 'crucial', 'role', 'process', 'clarifying', 'grammatical', 'functions', 'individual', 'words', 'tags', 'provide', 'information', 'sentence', 'structure', 'including', 'subjectverbobject', 'relationships', 'help', 'understanding', 'words', 'work', 'together', 'within', 'sentence', 'conversely', 'semantic', 'analysis', 'explores', 'meaning', 'individual', 'words', 'sentences', 'pos', 'tags', 'provide', 'useful', 'indicators', 'semantic', 'analysis', 'highlighting', 'word', 'grammatical', 'categories', 'example', 'distinguishing', 'nouns', 'verbs', 'significant', 'impact', 'one', 'understands', 'meaning', 'sentence', 'illuminating', 'subjects', 'actions', 'objects', '3', 'additionally', 'pos', 'tagging', 'helps', 'resolve', 'ambiguity', 'exists', 'natural', 'language', 'many', 'words', 'one', 'meaning', 'perform', 'differently', 'depending', 'situation', 'possible', 'reduce', 'ambiguity', 'improve', 'comprehension', 'interpretation', 'text', 'giving', 'words', 'proper', 'pos', 'tags', 'produce', 'meaningful', 'insights', 'text', 'data', 'language', 'understanding', 'tasks', 'machine', 'translation', 'sentiment', 'analysis', 'named', 'entity', 'recognition', 'mainly', 'depend', 'accurate', 'pointofsale', 'pos', 'information', 'reason', 'process', 'essential', 'furthermore', 'making', 'possible', 'identify', 'linguistic', 'components', 'like', 'proper', 'nouns', 'verb', 'phrases', 'adjectival', 'modifiers', 'pos', 'tags', 'aid', 'extraction', 'information', 'moreover', 'pos', 'tags', 'useful', 'features', 'training', 'models', 'machine', 'learning', 'applications', 'improving', 'performance', 'offering', 'crucial', 'contextual', 'information', 'accurate', 'predictions', 'various', 'nlp', 'tasks', '7', 'previous', 'literature', 'highlighted', 'challenges', 'associated', 'hmmbased', 'pos', 'tagging', 'especially', 'dealing', 'ambiguous', 'terms', 'complicated', 'sentence', 'patterns', '1', 'furthermore', 'growing', 'need', 'reliable', 'accurate', 'part', 'speech', 'pos', 'systems', 'due', 'growing', 'demand', 'nlp', 'systems', 'analyse', 'heterogeneous', 'unstructured', 'text', 'input2', 'study', 'intends', 'investigate', 'advanced', 'pos', 'tagging', 'techniques', 'specifically', 'conditional', 'random', 'fields', 'crfs', '10', 'integration', 'spacy', 'nlp', 'toolkit', 'motivated', 'inadequacies', 'existing', 'systems', 'crfs', 'provide', 'improved', 'sequence', 'modelling', 'capabilities', 'making', 'possible', 'include', 'contextual', 'data', 'effectively', 'depends', 'features', '2', 'integrating', 'spacy', 'makes', 'use', 'extensive', 'linguistic', 'features', 'domainspecific', 'expertise', 'improve', 'tagging', 'process', '15', 'paper', 'aims', 'analyse', 'two', 'things', '1', 'well', 'crfs', 'work', 'comparison', 'conventional', 'hmmbased', 'pos', 'tagging', 'techniques', '2', 'combining', 'crfs', 'spacy', 'affects', 'resilience', 'accuracy', 'pos', 'tagging', 'work', 'contributes', 'showing', 'additional', 'advantages', 'integrating', 'crfs', 'spacy', 'offering', 'actual', 'proof', 'superiority', 'crfs', 'hmms', 'pos', 'tagging', 'furthermore', 'provide', 'insights', 'difficulties', 'possibilities', 'utilising', 'cuttingedge', 'nlp', 'approaches', 'enhanced', 'linguistic', 'examination', '9', 'rest', 'paper', 'structured', 'follows', 'thorough', 'summary', 'relevant', 'work', 'field', 'pos', 'tagging', 'given', 'section', '2', 'approach', 'covered', 'section', '3', 'along', 'specifics', 'crfs', 'implemented', 'integrated', 'spacy', '16', 'experimental', 'setup', 'data', 'analysis', 'shown', 'section', '4', 'discussion', 'included', 'section', '5', 'section', '6', 'offers', 'final', 'thoughts', 'potential', 'directions', 'future', 'research', 'wrap', 'paper', '2024', 'third', 'international', 'conference', 'distributed', 'computing', 'electrical', 'circuits', 'electronics', 'icdcece', '9798350318609243100', '2024', 'ieee', 'doi', '101109icdcece60827202410548901', 'authorized', 'licensed', 'use', 'limited', 'amrita', 'school', 'engineering', 'downloaded', 'july', '232024', '052829', 'utc', 'ieee', 'xplore', 'restrictions', 'apply', 'fig', '1', 'part', 'speech', 'tagging', 'fig1', 'shows', 'example', 'parts', 'speech', 'tagging', 'done', 'sentence']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FPhWzRNRkUmV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}